---
title: "Chapter 2 Exercises"
output: html_document
date: "2025-01-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tibble)

```

# Conceptual

## Exercise 1
For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.

(a) The sample size n is extremely large, and the number of predictors p is small.
(b) The number of predictors p is extremely large, and the number of observations n is small.
(c) The relationship between the predictors and response is highly non-linear.
(d) The variance of the error terms, i.e. σ2 = Var(ε), is extremely high.

```{r exercise-1-solution}

# (a) The sample size n is extremely large, and the number of predictors p is small.
# Flexible statistical learning method would be better than an inflexible method.
# Justification: With a large sample size, a flexible method can take advantage of the large sample size to produce more accurate predictions.

# (b) The number of predictors p is extremely large, and the number of observations n is small.
# Inflexible statistical learning method would be better than a flexible method.
# Justification: With a small sample size, a flexible method may overfit the data and produce inaccurate predictions.

# (c) The relationship between the predictors and response is highly non-linear.
# Flexible statistical learning method would be better than an inflexible method.
# Justification: A flexible method can capture the non-linear relationship between the predictors and response, while an inflexible method may not be able to do so.

# (d) The variance of the error terms, i.e. σ2 = Var(ε), is extremely high.
# Inflexible statistical learning method would be better than a flexible method.
# Justification: A flexible method may overfit the data and produce inaccurate predictions when the variance of the error terms is extremely high.

```

## Exercise 2
Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.

(a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.

(b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.

(c) We are interested in predicting the % change in the US dollar in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the dollar, the % change in the US market, the % change in the British market, and the % change in the German market.

(d) We are interested in predicting the price of a stock. We collect data for the last month on the price of the stock at closing, the % change in the US market, the % change in the British market, and the % change in the German market.

```{r exercise-2-solution}
# (a) Regression; n = 500, p = 3; Inference
# (b) Classification; n = 20, p = 13; Prediction
# (c) Regression; n = 52, p = 3; Prediction
# (d) Regression; n = 20, p = 3; Prediction
```

## Exercise 3
We now revisit the bias-variance decomposition.

(a) Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or optimal) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.
(b) Explain why each of the five curves has the shape displayed in part (a).

```{r exercise-3-solution}

```


## Exercise 4
You will now think of some real-life applications for statistical learning.

(a) Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.

(b) Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.

(c) Describe three real-life applications in which cluster analysis might be useful.

```{r exercise-4-solution}
# a1
# scenario: whether or not a team wins a football game
# predictors: offensive and defensive statistics
# response: game w or l
# type: prediction

# a2
# scenario: whether or not a patient has a disease
# predictors: patient's medical history
# response: disease y or n
# type: prediction

# a3
# scenario: how likely a customer is to churn
# predictors: customer's purchase history
# response: churn y or n
# type: prediction/inference

# b1
# scenario: predicting the price of a house
# predictors: house size, location, etc.
# response: house price
# type: prediction

# b2
# scenario: predicting the number of sales for a company
# predictors: marketing budget, competition, etc.
# response: sales
# type: prediction

# b3
# scenario: determining the effect of a drug on a patient
# predictors: drug dosage, patient's medical history
# response: patient's health
# type: inference

# c1
# scenario: customer segmentation
# predictors: customer's purchase history
# response: customer segments
# type: clustering

# c2
# scenario: grouping similar states
# predictors: state population, GDP, etc.
# response: state groups
# type: clustering

# c3
# scenario: grouping similar products
# predictors: product features
# response: product groups
# type: clustering

```


## Exercise 5

What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?

```{r exercise-5-solution}
# Advantages of a very flexible approach:
# - Can capture complex relationships in the data
# - Can produce less bias

# Disadvantages of a very flexible approach:
# - Can produce high variance
# - Can overfit the data

# A more flexible approach might be preferred when:
# - The relationship between the predictors and response is highly non-linear
# - The sample size is large

# A less flexible approach might be preferred when:
# - The sample size is small
# - The relationship between the predictors and response is linear
```

## Exercise 6

Describe the differences between parametric and non-parametric statistical learning approaches. What are the advantages of a parametric approach to regression or classification (as opposed to a non-parametric approach)? What are its disadvantages?

```{r exercise-6-solution}
# Parametric statistical learning approaches make assumptions about the functional form of the relationship between the predictors and response, while non-parametric approaches do not make such assumptions.

# Advantages of a parametric approach:
# - Simplicity
# - Interpretability
# - Less computational cost

# Disadvantages of a parametric approach:
# - May not capture complex relationships in the data
# - May produce biased estimates if the assumptions are incorrect

```

## Exercise 7
The table below contains some training data for a set of six observations, three predictors, and one qualitative response variable.

| Obs. | X1 | X2 | X3 | Y |
|------|----|----|----|---|
| 1    | 0  | 3  | 0  | Red |
| 2    | 2  | 0  | 0  | Red |
| 3    | 0  | 1  | 3  | Red |
| 4    | 0  | 1  | 2  | Green |
| 5    | -1 | 0  | 1  | Green |
| 6    | 1  | 1  | 1  | Red |

Suppose we wish to use this data to make a prediction for Y when X1 = X2 = X3 = 0 using K-nearest neighbors.

(a) Compute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0.
(b) What is our prediction with K = 1? Why?
(c) What is our prediction with K = 3? Why?
(d) If the Bayes decision boundary in this problem is highly non-linear, would we expect the best value for K to be large or small? Why?

```{r exercise-7-solution}

data <- tribble( ~Obs., ~X1, ~X2, ~X3, ~Y, 1, 0, 3, 0, "Red", 2, 2, 0, 0, "Red", 3, 0, 1, 3, "Red", 4, 0, 1, 2, "Green", 5, -1, 0, 1, "Green", 6, 1, 1, 1, "Red" )

# (a) Compute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0.
data$distance <- sqrt((data$X1 - 0)^2 + (data$X2 - 0)^2 + (data$X3 - 0)^2)
print(data$distance)

# (b) What is our prediction with K = 1? Why?
# The closest observation to the test point is observation 5, which has a response of Green. Therefore, our prediction with K = 1 is Green.

# (c) What is our prediction with K = 3? Why?
# The three closest observations to the test point are observations 5, 2, and 6, which have responses of Green, Red, and Red, respectively. Therefore, our prediction with K = 3 is Red.

# (d) If the Bayes decision boundary in this problem is highly non-linear, would we expect the best value for K to be large or small? Why?


```

# Applied

## Exercise 8

(a) Use the read.csv() function to read the data into R. Call the loaded data `college`. Make sure that you have the directory set to the correct location for the data.

```{r exercise-8-solution-a}
library(readr)
college <- read_csv("datasets/College.csv")

```

(b) Look at the data using the View() function. You should notice that the first column is just the name of each university. We don’t really want R to treat this as data. However, it may be handy to have these names for later. 

```{r exercise-8-solution-b}

# get the length of the first column
n <- nrow(college)

# rename first column
names(college)[1] <- "University"

# convert college to dataframe
college <- as.data.frame(college)

# assign the first column as rownames
rownames(college) <- college$University

# remove the first column
college <- college[,-1]

```

